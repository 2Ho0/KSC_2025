import collections
import functools
from functools import partial as bind

import elements
import embodied
import numpy as np
import torch as t

from Decision_Transformer.src.decision_transformer.train import dt_inference
from Decision_Transformer.src.models.trajectory_transformer import (DecisionTransformer)
from Decision_Transformer.src.config import( EnvironmentConfig, TransformerModelConfig)
from Decision_Transformer.src.decision_transformer.offline_dataset import TrajectoryDataset

import jax
from gymnasium.spaces import Box, Discrete

  
def create_dreamer_batch_from_dt(dt_batch_np, reference_batch_jax):
    """DT ë°°ì¹˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Dreamer í•™ìŠµì— í•„ìš”í•œ ë°°ì¹˜ êµ¬ì¡°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    
    # ìƒˆë¡œìš´ ë°°ì¹˜ë¥¼ NumPy ë°°ì—´ë¡œ êµ¬ì„±í•  ë”•ì…”ë„ˆë¦¬
    new_batch_np = {}
    
    # 1. ì§ì ‘ ë§¤í•‘ ê°€ëŠ¥í•œ í‚¤ë“¤
    new_batch_np['image'] = dt_batch_np['states']
    new_batch_np['action'] = dt_batch_np['actions']
    # new_batch_np['reward'] = dt_batch_np['rewards']
    new_batch_np['is_last'] = dt_batch_np['is_last']
    new_batch_np['is_terminal'] = dt_batch_np['is_last'] # is_lastì™€ ë™ì¼í•˜ê²Œ ê°€ì •
    new_batch_np['episode_step'] = dt_batch_np['timesteps']
    # new_batch_np['task_id'] = dt_batch_np['task_ids']
    new_batch_np['rtg'] = dt_batch_np['returns_to_go']  # Dreamerì—ì„œ RTGë¥¼ rtgë¡œ ì‚¬ìš©

    is_last_shifted = np.roll(new_batch_np['is_last'], 1, axis=1)
    is_last_shifted[:, 0] = True
    new_batch_np['is_first'] = is_last_shifted
    
    # 3. Dreamerì— í•„ìˆ˜ì ì´ì§€ë§Œ DT ë°°ì¹˜ì—ëŠ” ì—†ëŠ” í‚¤ë“¤ (Zero-padding)
    # ëª¨ë¸ì˜ ë‚´ë¶€ ìƒíƒœ(dyn/*)ëŠ” ì •ë³´ê°€ ì—†ìœ¼ë¯€ë¡œ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ì—¬, 
    # "ì—¬ê¸°ì„œë¶€í„° ìƒˆë¡œìš´ ìƒìƒì„ ì‹œì‘í•˜ë¼"ëŠ” ì‹ í˜¸ë¥¼ ì¤ë‹ˆë‹¤.
    for key, value in reference_batch_jax.items():
        if key not in new_batch_np:
            # ì›ë³¸ ë°°ì¹˜ì˜ shapeê³¼ dtypeì„ ì°¸ì¡°í•˜ì—¬ 0ìœ¼ë¡œ ì±„ìš´ ë°°ì—´ ìƒì„±
            new_batch_np[key] = np.zeros(value.shape, dtype=value.dtype)

    # 4. NumPy ë”•ì…”ë„ˆë¦¬ë¥¼ JAX ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
    return jax.tree_util.tree_map(jax.device_put, new_batch_np)
    # return new_batch_np

def train(make_agent, make_replay, make_env, make_stream, make_logger, args):
  task_manager = None
  if isinstance(make_env, functools.partial) and 'task_manager' in make_env.keywords:
      task_manager = make_env.keywords['task_manager']
  agent = make_agent()
  logger = make_logger()

  # Dreamer í•™ìŠµì„ ìœ„í•œ í†µí•© ë¦¬í”Œë ˆì´ ë²„í¼ (ëª¨ë“  íƒœìŠ¤í¬ ë°ì´í„°ê°€ ì—¬ê¸°ì— ëª¨ì„)
  replay = make_replay() 
  dt_replay = make_replay(mode='decision_transformer')

  logdir = elements.Path(args.logdir)
  step = logger.step
  usage = elements.Usage(**args.usage)
  train_agg = elements.Agg()
  epstats = elements.Agg()
  episodes = collections.defaultdict(elements.Agg)
  policy_fps = elements.FPS()
  train_fps = elements.FPS()
  episode_buffers = collections.defaultdict(list)

  batch_steps = args.batch_size * args.batch_length
  should_train = elements.when.Ratio(args.train_ratio / batch_steps)
  should_log = embodied.LocalClock(args.log_every)
  should_report = embodied.LocalClock(args.report_every)
  should_save = embodied.LocalClock(args.save_every)

  # Decision Transformer ëª¨ë¸ ë° ê´€ë ¨ ì„¤ì • ì´ˆê¸°í™” (í•™ìŠµ ë£¨í”„ ì‹œì‘ ì „ 1íšŒ ì‹¤í–‰)
  dt_env_config = EnvironmentConfig()
  dt_model_config = TransformerModelConfig()

  # DTìš© í™˜ê²½ì€ DT ëª¨ë¸ ë‚´ë¶€ì˜ observation/action space ì •ë³´ë¥¼ ì„¤ì •í•˜ê¸° ìœ„í•´ í•œ ë²ˆë§Œ ìƒì„±í•©ë‹ˆë‹¤.
  dt_env = make_env(0)  # 'episode_step' ì¶”ê°€
  act_space = dt_env.act_space
  
  # embodiedì˜ ë”•ì…”ë„ˆë¦¬ í˜•íƒœì—ì„œ ì‹¤ì œ Space ê°ì²´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
  main_action_space = act_space['action']
  num_actions = int(main_action_space.high) # .highê°€ í–‰ë™ì˜ ê°œìˆ˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
  dt_env_config.action_space = Discrete(num_actions)
  mock_obs_space = Box(low=0, high=255, shape=(6, 6, 3), dtype=np.uint8)
  dt_env_config.observation_space = mock_obs_space
  
  dt_model = DecisionTransformer(
      environment_config=dt_env_config,
      transformer_config=dt_model_config,
  )
  model_path = '/home/hail/Desktop/Alarm/models/8377.pt'
  checkpoint = t.load(model_path)
  dt_model.load_state_dict(checkpoint['model_state_dict'])

  def calculate_rtg(rewards, gamma=0.99):
    """ì£¼ì–´ì§„ ë³´ìƒ ì‹œí€€ìŠ¤ë¡œ Returns-To-Goë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    rtgs = []
    discounted_reward = 0
    for r in reversed(rewards):
        discounted_reward = r + gamma * discounted_reward
        rtgs.insert(0, discounted_reward)
    return np.array(rtgs, dtype=np.float32)

  def collect_and_process_episode(tran, worker):
      # 1. í˜„ì¬ ìŠ¤í…ì˜ ë°ì´í„°ë¥¼ í•´ë‹¹ ì›Œì»¤ì˜ ì„ì‹œ ë²„í¼ì— ì¶”ê°€
      episode_buffers[worker].append(tran)

      buffer_len = len(episode_buffers[worker])
      if buffer_len % 700 == 0: # 700 ìŠ¤í…ë§ˆë‹¤ ì¶œë ¥
          print(f"DEBUG: Worker {worker}, episode_buffer length: {buffer_len}")
      
      # 2. ì—í”¼ì†Œë“œê°€ ëë‚˜ì§€ ì•Šì•˜ìœ¼ë©´ ì•„ë¬´ê²ƒë„ í•˜ì§€ ì•Šê³  ì¢…ë£Œ
      if not tran['is_last']:
          return

      # 3. ì—í”¼ì†Œë“œê°€ ëë‚¬ìœ¼ë©´, í•´ë‹¹ ì›Œì»¤ì˜ ë²„í¼ì—ì„œ ëª¨ë“  ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
      episode_data = episode_buffers.pop(worker)
      
      # 4. ë³´ìƒ(reward)ë§Œ ì¶”ì¶œí•˜ì—¬ RTG ê³„ì‚°
      rewards = [step['reward'] for step in episode_data]
      rtgs = calculate_rtg(rewards) # RTG ê³„ì‚° í•¨ìˆ˜ í˜¸ì¶œ

      # TaskManagerë¡œë¶€í„° í˜„ì¬ í™œì„± íƒœìŠ¤í¬ì˜ ì¸ë±ìŠ¤ë¥¼ ê°€ì ¸ì˜´
      current_task_index = task_manager.current_task_idx % len(task_manager.tasks)
      
      # 5. ì—í”¼ì†Œë“œì˜ ëª¨ë“  ìŠ¤í…ì— ëŒ€í•´ RTGì™€ ì¶”ê°€ ì •ë³´ë¥¼ ë¶™ì—¬ ë¦¬í”Œë ˆì´ ë²„í¼ì— ì¶”ê°€
      for i, step_data in enumerate(episode_data):
          # tranì€ ìˆ˜ì •í•˜ë©´ ì•ˆ ë˜ë¯€ë¡œ ë³µì‚¬ë³¸ì„ ë§Œë“¦
          labeled_tran = dict(step_data)
          
          # ê³„ì‚°ëœ RTGì™€ íƒœìŠ¤í¬ ID, ê¸€ë¡œë²Œ ìŠ¤í…ì„ ì¶”ê°€
          labeled_tran['rtg'] = np.array([rtgs[i]], dtype=np.float32)
          labeled_tran['task_id'] = np.array(current_task_index, dtype=np.int32)
          labeled_tran['global_step'] = np.array(step.value, dtype=np.int64)
          
          # ë¼ë²¨ì´ ë¶™ì€ ë°ì´í„°ë¥¼ ê° ë¦¬í”Œë ˆì´ ë²„í¼ì— ì¶”ê°€
          replay.add(labeled_tran, worker)
          dt_replay.add(labeled_tran, worker)

  @elements.timer.section('logfn')
  def logfn(tran, worker):
    episode = episodes[worker]
    tran['is_first'] and episode.reset()
    episode.add('score', tran['reward'], agg='sum')
    episode.add('length', 1, agg='sum')
    episode.add('rewards', tran['reward'], agg='stack')
    for key, value in tran.items():
      if value.dtype == np.uint8 and value.ndim == 3:
        if worker == 0:
          # episode.add(f'policy_{key}', value, agg='stack')
          continue
      elif key.startswith('log/'):
        assert value.ndim == 0, (key, value.shape, value.dtype)
        episode.add(key + '/avg', value, agg='avg')
        episode.add(key + '/max', value, agg='max')
        episode.add(key + '/sum', value, agg='sum')
    if tran['is_last']:
      result = episode.result()
      logger.add({
          'score': result.pop('score'),
          'length': result.pop('length'),
      }, prefix='episode')
      rew = result.pop('rewards')
      if len(rew) > 1:
        result['reward_rate'] = (np.abs(rew[1:] - rew[:-1]) >= 0.01).mean()
      epstats.add(result)

      if task_manager:
        # 1. TaskManagerì— ì—í”¼ì†Œë“œ ì¢…ë£Œë¥¼ ì•Œë¦½ë‹ˆë‹¤.
        task_manager.on_episode_end(
            episode_reward=result.get('score', 0),
            episode_length=result.get('length', 0)
        )
        # 2. íƒœìŠ¤í¬ë¥¼ ì „í™˜í•´ì•¼ í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        if task_manager.should_switch_task():
          old_task = task_manager.get_current_task_name()
          task_manager.switch_task()
          new_task = task_manager.get_current_task_name()
          print(f"ğŸ”„ Task ì „í™˜: {old_task} -> {new_task}. í™˜ê²½ì„ ë¦¬ì…‹í•©ë‹ˆë‹¤.")

          # # 3. ëª¨ë“  í™˜ê²½ì„ ë¦¬ì…‹í•˜ì—¬ ìƒˆë¡œìš´ íƒœìŠ¤í¬ë¥¼ ì ìš©í•©ë‹ˆë‹¤.
          # driver.reset(agent.init_policy)
          
          # # 4. ìƒˆë¡œìš´ íƒœìŠ¤í¬ì— ëŒ€í•œ í†µê³„ë¥¼ ìƒˆë¡œ ê¸°ë¡í•˜ê¸° ìœ„í•´ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
          # episodes.clear()
          # epstats.reset()
          # carry_train[0] = agent.init_train(args.batch_size)

  fns = [bind(make_env, i, task_manager=task_manager) for i in range(args.envs)]
  driver = embodied.Driver(fns, parallel=not args.debug)
  driver.on_step(lambda tran, _: step.increment())
  driver.on_step(lambda tran, _: policy_fps.step())
  driver.on_step(collect_and_process_episode) 
  driver.on_step(logfn)

  stream_train = iter(agent.stream(make_stream(replay, 'train')))
  stream_report = iter(agent.stream(make_stream(replay, 'report')))

  carry_train = [agent.init_train(args.batch_size)]
  carry_report = agent.init_report(args.batch_size)

  def trainfn(tran, worker):
    if len(replay) < args.batch_size * args.batch_length:
      return
    for _ in range(should_train(step)):
      with elements.timer.section('stream_next'):
        batch = next(stream_train)
      # Decision Transformerë¡œ Task Shift ê°ì§€
      with elements.timer.section('dt_task_shift'):
        dt_batch = dt_replay.sample(args.batch_size)
        # batch_dt = jax.device_get(dt_batch)
        batch_dt = dt_batch
        
        B, T = batch_dt['is_first'].shape
        
        dt_batch_data = {
            'states': batch_dt['image'], # (64,21,6,6,3)
            'actions': batch_dt['action'], # (64,21)
            # 'rewards': batch_dt['reward'], # (64,21)
            'returns_to_go': batch_dt['rtg'], # (64,21)
            # 'attention_mask': ~batch_dt['is_last'],
            # 'attention_mask': np.tril(np.ones((T, T))).astype(bool), # (21,21)
            'timesteps': batch_dt['episode_step'], # (64,21)
            # 'task_ids': batch_dt['task_id'], # (64,21)
            'is_last': batch_dt['is_last'] # (64,21)
        }
        print("returns_to_go:", dt_batch_data['returns_to_go'].max())
        
        task_shift = dt_inference(
              model=dt_model,
              trajectory_data_set=dt_batch_data,
              num_actions=num_actions
          )
        
        current_task_in_batch = int(np.bincount(batch_dt['task_id'].flatten()).argmax())
        
        if task_shift:
            # ìœ„ì—ì„œ ì •ì˜í•œ í—¬í¼ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ DT ë°°ì¹˜ë¥¼ Dreamer í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            train_batch = create_dreamer_batch_from_dt(dt_batch_data, batch)
            # train_batch = batch_dt
            train_batch['seed'] = batch['seed']
            train_batch['consec'] = batch['consec']
            carry_train[0], outs, mets = agent.train_real(carry_train[0], train_batch)
        else:
            # Task Shiftê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ Dreamer ë°°ì¹˜ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            train_batch = batch
            carry_train[0], outs, mets = agent.train(carry_train[0], train_batch)
        
        train_fps.step(batch_steps)
        if 'replay' in outs:
          replay.update(outs['replay'])
        train_agg.add(mets, prefix='train')

  driver.on_step(trainfn)

  cp = elements.Checkpoint(logdir / 'ckpt')
  cp.step = step
  cp.agent = agent
  cp.replay = replay
  if args.from_checkpoint:
    elements.checkpoint.load(args.from_checkpoint, dict(
        agent=bind(agent.load, regex=args.from_checkpoint_regex)))
  cp.load_or_save()

  print('Start training loop')
  policy = lambda *args: agent.policy(*args, mode='train')
  driver.reset(agent.init_policy)
  while step < args.steps:

    driver(policy, steps=100)

    if should_report(step) and len(replay):
      agg = elements.Agg()
      for _ in range(args.consec_report * args.report_batches):
        batch_report = next(stream_report)
        current_task_index = task_manager.current_task_idx
        B, T_rep = batch_report['is_first'].shape
        # task_shift_result: ë¦¬í¬íŠ¸ì—ì„  Falseë¡œ ì±„ì›Œë„ OK
        # batch_report['task_shift_result'] = np.zeros((B, T_rep, T_rep), dtype=bool)
        # task_id: í˜„ì¬ íƒœìŠ¤í¬ IDë¡œ ì±„ì›€
        batch_report['task_id'] = np.full((B, T_rep), int(current_task_index), dtype=np.int32)

        carry_report, mets = agent.report(carry_report, batch_report)
        agg.add(mets)
      logger.add(agg.result(), prefix='report')

    if should_log(step):
      logger.add(train_agg.result())
      logger.add(epstats.result(), prefix='epstats')
      logger.add(replay.stats(), prefix='replay')
      logger.add(usage.stats(), prefix='usage')
      logger.add({'fps/policy': policy_fps.result()})
      logger.add({'fps/train': train_fps.result()})
      logger.add({'timer': elements.timer.stats()['summary']})
      logger.write()

    if should_save(step):
      cp.save()

  logger.close()